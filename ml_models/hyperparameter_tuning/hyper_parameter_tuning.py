"""
Hyperparameter Tuning Module for Stock Selection Models

This module provides hyperparameter tuning functionalities using Grid Search,
Randomized Search, and Bayesian Optimization for machine learning models. It is
designed to work with your stock selection dataframe (final_merged_factors.csv)
which is generated by your pipeline.

Usage:
    1. Ensure that final_merged_factors.csv exists in the project root.
    2. Run this module directly to tune the chosen ML model.
       The example below tunes a RandomForestClassifier using accuracy as the metric.

You can adjust the model, parameter grid, and scoring metric as needed.
"""

import warnings
warnings.filterwarnings("ignore", category=UserWarning)

import pandas as pd
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

# Attempt to import BayesSearchCV for Bayesian optimization.
try:
    from skopt import BayesSearchCV
except ImportError:
    BayesSearchCV = None


class HyperparameterTuner:
    def __init__(self, model, param_grid, scoring='accuracy', cv=5, n_jobs=-1):
        """
        Initialize the HyperparameterTuner with a model and parameter grid.

        :param model: Scikit-learn estimator to tune.
        :param param_grid: Dictionary with parameter names as keys and lists (or skopt search spaces)
                           of parameter settings to try.
        :param scoring: Scoring metric to evaluate predictions (default 'accuracy').
        :param cv: Number of cross-validation folds.
        :param n_jobs: Number of jobs to run in parallel.
        """
        self.model = model
        self.param_grid = param_grid
        self.scoring = scoring
        self.cv = cv
        self.n_jobs = n_jobs

    def tune_with_grid_search(self, X, y):
        """
        Perform hyperparameter tuning using Grid Search.

        :param X: Feature dataset.
        :param y: Target variable.
        :return: Tuple (best_estimator, best_score, best_params)
        """
        grid_search = GridSearchCV(
            self.model,
            self.param_grid,
            scoring=self.scoring,
            cv=self.cv,
            n_jobs=self.n_jobs
        )
        grid_search.fit(X, y)
        return grid_search.best_estimator_, grid_search.best_score_, grid_search.best_params_

    def tune_with_random_search(self, X, y, n_iter=50):
        """
        Perform hyperparameter tuning using Randomized Search.

        :param X: Feature dataset.
        :param y: Target variable.
        :param n_iter: Number of parameter settings to sample.
        :return: Tuple (best_estimator, best_score, best_params)
        """
        random_search = RandomizedSearchCV(
            self.model,
            self.param_grid,
            scoring=self.scoring,
            cv=self.cv,
            n_jobs=self.n_jobs,
            n_iter=n_iter,
            random_state=42
        )
        random_search.fit(X, y)
        return random_search.best_estimator_, random_search.best_score_, random_search.best_params_

    def tune_with_bayesian_optimization(self, X, y, n_iter=50):
        """
        Perform hyperparameter tuning using Bayesian Optimization.

        :param X: Feature dataset.
        :param y: Target variable.
        :param n_iter: Number of iterations for optimization.
        :return: Tuple (best_estimator, best_score, best_params)
        :raises ImportError: if scikit-optimize is not installed.
        """
        if BayesSearchCV is None:
            raise ImportError("scikit-optimize is not installed. Install it to use Bayesian optimization.")
        bayes_search = BayesSearchCV(
            self.model,
            self.param_grid,
            scoring=self.scoring,
            cv=self.cv,
            n_jobs=self.n_jobs,
            n_iter=n_iter,
            random_state=42
        )
        bayes_search.fit(X, y)
        return bayes_search.best_estimator_, bayes_search.best_score_, bayes_search.best_params_


if __name__ == "__main__":
    # Load the final merged factors dataframe from your pipeline.
    try:
        df = pd.read_csv("final_merged_factors.csv")
    except Exception as e:
        print(f"Error loading final_merged_factors.csv: {e}")
        exit(1)

    # Convert date column to datetime if present.
    if "date" in df.columns:
        df["date"] = pd.to_datetime(df["date"])

    # Ensure the dataframe contains the target column.
    if "target" not in df.columns:
        print("The dataframe does not contain a 'target' column. Exiting.")
        exit(1)

    # Prepare the feature matrix (X) and target vector (y). Exclude non-feature columns.
    X = df.drop(columns=["date", "Ticker", "target"], errors="ignore")
    y = df["target"]

    # Import the desired ML model.
    # In this example, we tune the underlying estimator of RandomForestModel.
    from ml_models.models_ml.random_forest import RandomForestModel
    rf_model = RandomForestModel()
    estimator = rf_model.model  # This is a RandomForestClassifier instance.

    # Define a parameter grid for tuning.
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [None, 5, 10],
        'min_samples_split': [2, 5, 10]
    }

    # Instantiate the HyperparameterTuner.
    tuner = HyperparameterTuner(estimator, param_grid, scoring='accuracy', cv=5, n_jobs=-1)

    # Perform tuning using Grid Search.
    best_estimator, best_score, best_params = tuner.tune_with_grid_search(X, y)
    print("Grid Search - Best Score:", best_score)
    print("Grid Search - Best Params:", best_params)

    # Perform tuning using Randomized Search.
    best_estimator, best_score, best_params = tuner.tune_with_random_search(X, y, n_iter=20)
    print("Randomized Search - Best Score:", best_score)
    print("Randomized Search - Best Params:", best_params)

    # Perform tuning using Bayesian Optimization if available.
    try:
        best_estimator, best_score, best_params = tuner.tune_with_bayesian_optimization(X, y, n_iter=20)
        print("Bayesian Optimization - Best Score:", best_score)
        print("Bayesian Optimization - Best Params:", best_params)
    except ImportError as e:
        print(e)
